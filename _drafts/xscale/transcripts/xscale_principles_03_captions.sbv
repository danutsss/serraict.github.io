0:00:01.390,0:00:06.600
Hello everyone welcome to the third X scale Alliance webinar on the principles of agile

0:00:07.299,0:00:08.590
organization

0:00:08.590,0:00:12.210
My name is Peter Merrill in our previous webinars

0:00:12.210,0:00:18.569
We discussed stacking growth curves for exponential return on investment and the principle and practice of designing for simplicity

0:00:19.660,0:00:27.449
today we'll look at how an agile organization prioritizes design to continuously optimize throughput a goal that Ellie goldrich described as

0:00:27.760,0:00:29.619
opening the bottleneck

0:00:29.619,0:00:36.058
Go--let was the father of throughput accounting an alternative to cost accounting with general application throughout the worlds of business and engineering

0:00:38.079,0:00:43.558
Anything with the word accounting and it tends to turn Agilent's off, but if you've an agile mindset

0:00:43.559,0:00:46.199
I promise this throughput stuff will astonish you

0:00:47.530,0:00:55.050
You're looking at a Gantt chart the backbone of traditional management and cost accounting it represents three kinds of constraint time

0:00:55.449,0:00:57.449
operating expense and scope

0:00:57.789,0:01:03.119
The most constrained part of this chart is called the critical path, and it's colored in crimson here

0:01:04.030,0:01:10.140
Goldratt said business decisions must account for many more constraints than just those three

0:01:11.680,0:01:13.680
This Theory of Constraints

0:01:13.810,0:01:19.740
Says that in any system of production at any time. There is always a single dominant constraint

0:01:20.680,0:01:22.680
Gold rep called it the bottleneck

0:01:22.930,0:01:29.009
By definition any work done to open a constraint that isn't currently the bottleneck can't affect throughput

0:01:30.850,0:01:36.519
As throughput doesn't turn up on a Gantt chart and actually most constraints don't turn up on a Gantt chart

0:01:37.130,0:01:40.479
Gantt charts can't possibly generate logical business decisions

0:01:41.479,0:01:43.479
Let's look at where they go off the rails

0:01:43.670,0:01:51.519
This is an agile burndown chart the horizontal axis represents time the vertical represents delivery of valuable changes to system acres

0:01:52.070,0:01:55.989
the green curve represents a team's progress in actually doing this

0:01:57.440,0:02:03.399
Because humans aren't very good at predicting the future this green. Curve is wildly unrealistic

0:02:04.009,0:02:06.579
Teams often see something more like this blue one

0:02:08.720,0:02:16.209
Discovering their progress isn't going to satisfy expectations raised by their initial estimates the team begins working weekends and evenings

0:02:16.940,0:02:21.279
fatigue causes errors and rework that soon kills any gain in productivity

0:02:21.980,0:02:26.200
Management adds devs to help pick up the pace, which triggers Brookes law

0:02:27.769,0:02:31.629
Adding resource to a late project makes it later a

0:02:33.019,0:02:39.099
Smarter team might experience this purple curve seeing that their throughput is constrained. They reflect on the bottleneck

0:02:39.950,0:02:46.509
Let's say that they're a software delivery team who started out with pure scrum their productivity is getting killed by manual testing

0:02:47.390,0:02:51.850
They decide to invest time introducing BDD heroically hitting the restaurants

0:02:53.150,0:02:57.820
The problem with a Gantt chart is it confuses these two very different behaviors

0:02:58.010,0:03:02.049
It doesn't provide any reason to behave logically like the team on the purple curve

0:03:03.910,0:03:10.020
Determined to hit the green curve fits regrettably still commonplace for managers to whip their teams onto the blue one

0:03:11.470,0:03:16.470
Let's turn this burn down shot into a burn up chart so we can understand what's going on

0:03:16.840,0:03:19.350
Continuously and not just up to a fixed release date

0:03:19.350,0:03:25.619
But let's look at another possible behavior for the team which is better the red curve or the green curve

0:03:27.250,0:03:33.270
Well if the green curve is possible this part of the red curve must represent some kind of waste

0:03:33.550,0:03:40.589
which the lean guys called muda it might be inevitable waste perhaps caused by a dependency on an upstream team or

0:03:40.959,0:03:42.959
an influenza virus and

0:03:43.360,0:03:46.260
This part of the curve which lean calls Murray

0:03:46.959,0:03:54.059
Represents overburden we know it's overburdened because the team wasn't able to maintain its velocity and fell back to the black line

0:03:55.780,0:04:00.510
Typically overburden and waste occur in cycles like a drunk staggering from step to step

0:04:01.120,0:04:05.520
Lean calls this mirror, which means irregularity or wobble

0:04:06.700,0:04:10.649
The agile manifesto stands against this with its principle of sustainable pace

0:04:11.769,0:04:14.789
Clearly the green curve is a lot less wobbly than the red one

0:04:15.519,0:04:21.959
Indeed it's leaner but both curves meet expectations in the end, so why do we think the red one is worse?

0:04:25.330,0:04:29.879
Periods of overburden inevitably involve longer hours and increased stress

0:04:30.640,0:04:37.529
Corners may not be intentionally cut but under such conditions. It's inevitable that some design decisions are made in haste a

0:04:39.970,0:04:46.080
Repeating cycle of overburden makes short cuts and mistakes mound up to generate technical debt

0:04:47.950,0:04:53.189
Tech debt earns interest in the form of an exponential increase in the cost of change

0:04:54.220,0:05:00.720
Agile delivery is only sustainable when we continuously pay down this debt through merciless refactoring

0:05:01.900,0:05:07.620
But there's something strange about this now no matter how lean the curve there's less throughput

0:05:08.230,0:05:15.960
Just like the Gantt chart before it the burn-up chart hides a logical flaw in the way. We're measuring things that will inevitably lead to

0:05:16.630,0:05:18.630
illogical behavior

0:05:18.760,0:05:22.619
This floor has to do with the very definition of throughput

0:05:24.490,0:05:27.810
Controlling throughput is well understood in the domain of industrial design

0:05:28.450,0:05:35.400
This is Maxwell's governor from the 19th century the faster the steam exits the pipe the faster the ball spin round

0:05:35.710,0:05:43.319
Centripetal force moves the Assembly of levers to throttle the pipe resulting in a continuously sustained flow no matter how hot the fire

0:05:45.040,0:05:48.270
This is Don wells 20th century diagram of extreme programming

0:05:49.150,0:05:53.339
XP was inspired by Beck's famous desire to turn all the knobs to 10

0:05:54.220,0:06:01.110
These are the knobs they're all feedback loops like the steam governor. They work to generate a continuously sustained flow of code

0:06:02.080,0:06:06.659
By measuring the delivery of story points over time we can project release dates

0:06:09.780,0:06:15.979
Stories are continuously integrated into the codebase and the whole is continuously refactored to minimize technical debt

0:06:18.770,0:06:22.720
Well which one of these curves represents actual business throughput

0:06:24.680,0:06:28.150
Wait not so fast this is more to put on this dashboard

0:06:30.910,0:06:34.499
Obviously the work of delivering stories has to fit inside operating expense

0:06:36.880,0:06:43.709
Features are groups of stories that provide customer value as a whole features are the atoms of release planning

0:06:44.349,0:06:48.599
Figuring out their budgets and priorities is also work that draws from operating expense

0:06:49.780,0:06:54.750
We must continuously adapt feature acceptance tests to learnings from design and analytic work

0:06:56.890,0:07:04.590
Whenever these learnings mean the story burner won't fit within the feature budgets of the release plan we need to refactor the release plan

0:07:05.080,0:07:07.080
This requires whole board thinking

0:07:07.630,0:07:09.630
reprioritizing for maximum return

0:07:12.490,0:07:16.319
So we enter the realm of cost accounting where to describe throughput

0:07:16.360,0:07:23.430
We need only understand one number the return on the investment or in other words the cost per unit of revenue

0:07:27.410,0:07:29.980
Oops remember this guy from our first webinar

0:07:31.040,0:07:37.420
exponential and then entropic return on investment how do we relate this to all the jagged curves that we're just looking at

0:07:38.870,0:07:46.059
In our second webinar we saw how the return on a product breaks out into five metrics on its service ecosystem now

0:07:46.060,0:07:50.890
Let's get a little bit more empirical with these and provide an example of how we could measure and project them

0:07:52.300,0:07:54.900
You can pause here and go through this table at your leisure

0:07:55.300,0:08:01.469
The idea is simply that if on average 10% of the people you acquire wind up generating 10 dollars of return

0:08:01.810,0:08:04.620
You can value each person you acquire at $1

0:08:06.660,0:08:10.969
For that simplistic and these curves are far smoother than any we actually measure

0:08:11.190,0:08:15.350
But this little model serves to help us understand these things as constraints

0:08:16.050,0:08:22.669
if we have the number of customers we acquire that has the number we can activate and retain and the rate of referrals and the

0:08:22.669,0:08:23.910
amount of revenue

0:08:23.910,0:08:26.749
This is customer acquisition as a bottleneck

0:08:28.380,0:08:30.120
The same effect occurs when

0:08:30.120,0:08:36.080
Activation is the bottleneck while acquiring more customers may bump the rest up linearly if the majority are being lost through

0:08:36.479,0:08:41.299
Failing to activate. There's little point in acquiring more. We'll just poison the market that way

0:08:42.659,0:08:44.718
likewise if the bottleneck is in retention

0:08:45.240,0:08:52.609
Work done on acquisition or activation features won't stop the bleeding and work on referral and revenue features can't achieve any

0:08:52.830,0:08:54.830
significant effect

0:08:55.140,0:08:59.839
Referrals dominate the rate at which an ecosystem can grow and thereby its revenue and

0:09:00.390,0:09:04.429
shortcomings in initial revenue generating experiences may bottleneck the subsequent ones

0:09:07.740,0:09:11.930
So it's clear now that there's no such thing as a fixed cost per unit of revenue

0:09:13.230,0:09:21.020
Instead throughput is about the area under the curve, and this is the jumping off point for gold rats method of throughput accounting

0:09:22.620,0:09:27.289
Just like in cost accounting we have revenue and operating expense the Theory of Constraints

0:09:27.690,0:09:32.359
Gives us some new names for some other cost accounting terminology, but that's not the point

0:09:33.780,0:09:40.399
Golda distinguishes between operating expenses and what he calls truly variable costs things whose costs we can project

0:09:40.530,0:09:43.309
But which can't be improved by any work we do

0:09:44.840,0:09:47.199
What's left is the net profit if

0:09:47.630,0:09:53.200
revenue represents customer value and operating expense represents employee value net profit

0:09:53.450,0:09:58.150
Represents the owners value they balance this against their investment in establishing the business

0:10:00.020,0:10:06.850
So throughput accounting defines throughput as net profit plus operating expense, and that's what we've been missing

0:10:06.850,0:10:09.009
That's what we're actually trying to optimize

0:10:11.820,0:10:14.269
Alright, then let's see what difference it makes oh?

0:10:15.060,0:10:22.250
Look at that nasty blue system integration testing curve at the bottom of the operating expense you can see it's going exponential

0:10:23.279,0:10:29.509
So now costs are spiraling and technical debt threatens to eat up all the delivery bandwidth and kill productivity

0:10:30.269,0:10:33.559
That's gonna make a nasty constraint well. Let's fix it

0:10:35.399,0:10:41.839
Not only was that good for productivity, but by reducing staff costs it was good for the net profit to

0:10:42.390,0:10:45.890
Accept did you notice it didn't change throughput at all?

0:10:56.270,0:11:00.309
In fact this is the cost accounting equivalent of a farmer eating his seed corn

0:11:00.310,0:11:07.479
And it's a common misunderstanding of the lien principles if our first focus is reducing operating expense to the bare minimum

0:11:07.480,0:11:13.209
We need for efficient delivery will never retain the learning or the bandwidth we have to have to lift throughput

0:11:15.320,0:11:22.809
Instead of cutting expense we should exploit the fact that no constraint other than the immediate bottleneck can have a significant effect on throughput

0:11:23.000,0:11:30.399
that means if we simply maintain operating expense we can redirect a lot of resource into research and design work to

0:11:30.500,0:11:37.659
Generate the innovation we need to open the market bottleneck which ever pirate metric represents that at the moment

0:12:02.250,0:12:07.429
In fact we'll need to run market analytics continuously so we can respond immediately

0:12:07.920,0:12:14.660
To whatever constraint goes bottleneck from time to time. That's always the critical priority for design

0:12:16.200,0:12:21.289
But now we've lifted this first bottleneck perhaps, this is the time to minimize the operating expense

0:12:22.170,0:12:29.840
On the other hand by definition once. We've lifted one bottleneck a different constraint becomes a new bottleneck

0:12:31.290,0:12:37.550
We'd only cut operating expense if we had no more market or solution or organization bottlenecks to open

0:12:37.920,0:12:42.289
If anything our owners should be motivated to reinvest their net profits to increase

0:12:42.780,0:12:46.399
Operating expense in order to accelerate the attack on the next bottleneck

0:12:47.790,0:12:52.130
This might be a geographic constraint or segment constraint on our market

0:12:52.130,0:12:59.780
We can prioritize design to lift those or go the full Steve Jobs and prioritize designed to open a whole new service ecosystem

0:13:00.060,0:13:05.060
Leaving our competition to struggle in old markets while we sail for new ones

0:13:27.180,0:13:33.380
Before we get entirely carried away with such a romantic view let's consider throughput numbers in a different light

0:13:33.540,0:13:35.540
As we saw in our second webinar

0:13:35.610,0:13:41.419
We must use science as an integral part of our design process to validate assumptions as we go

0:13:41.850,0:13:49.579
This isn't about some kind of Big Bang magical thinking. It's about iterative reductive build measure learn and refactor

0:13:50.190,0:13:53.720
But this enlarge a problem an agile organization must solve

0:13:54.570,0:14:02.479
Certainly a clear-eyed scientific and continuous numerical understanding of throughput provides a much better basis for business decisions than with cost accounting

0:14:03.240,0:14:09.680
But that's assuming our organization has a logical process of decision making that can be learn from these analytics and metrics

0:14:11.370,0:14:16.250
Unfortunately here in the first half of the 21st century this isn't a safe assumption

0:14:19.380,0:14:27.320
Happily there's a good edge our solution to this proper one our HR delivery teams use every day without even thinking about it

0:14:27.320,0:14:32.840
And one that Hiawatha proved can scale to hundreds of thousands and operate sustainably for hundreds of years

0:14:34.200,0:14:39.290
We'll explore this territory in our fourth webinar on the principles of agile organization

0:14:39.990,0:14:42.530
autonomous teams in whole arc extremes

0:14:43.650,0:14:49.129
If you'd like to get involved more deeply with these ideas try XScale Alliance dot o-r-g

0:14:49.890,0:14:52.009
We look forward to your company next time
